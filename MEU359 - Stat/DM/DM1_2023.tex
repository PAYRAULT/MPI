\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage[margin=2cm,top=2.5cm,headheight=16pt,headsep=0.1in,heightrounded]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
%\usepackage{tikz}
\usepackage{pgfplots}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\E}{\bb{E}}
\newcommand{\C}{\bb{C}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;} 
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}
\newcommand{\vect}[1]{\overrightarrow{#1}}

%Pagination stuff.
%\setlength{\oddsidemargin}{0in}
%\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\cfoot{page \thepage}
\lhead{MEU302 - Alg\`ebre}
\rhead{TD2}
\pagestyle{fancy}


\begin{document}

\subsection*{Exercice 5}
\subsection*{Question 1}
On a

$$
E(X_1) = \int_{a}^{1}{t f(t) \,dt} = \int_{a}^{1}{t\frac{1}{1-a} \,dt} = \frac{1}{1-a}\frac{1-a^2}{2} = \frac{1+a}{2}
$$

On a
$$
E[X_1^2] = \int_{a}^{1}{t^2 f(t) \,dt} = \int_{a}^{1}{t^2\frac{1}{1-a} \,dt} = \frac{1}{1-a}\frac{1-a^3}{3} = \frac{1+a+a^2}{3}
$$

Donc
$$
V(X_1) = E[X_1^2]âˆ’(E[X_1])^2 = \frac{1+a+a^2}{3} -  \left( \frac{1+a}{2} \right)^2 = \frac{(1-a)^2}{12}
$$

\subsection*{Question 2}
On a
$$E[X_1] = \frac{1+a}{2} \implies a = 2E[X_1] - 1$$
Donc on prend comme EMM de $a$
$$
\tilde{a}_n = 2 \bar{a}_n - 1
$$
Mais $0 < a <1$, il faut donc que son estimateur soit aussi $0 < \tilde{a}_n <1$
Donc 
$$
0 < 2\bar{a}_n - 1 < 1 \implies 1 < 2\bar{a}_n < 2 \implies 1/2 < \bar{a}_n < 1
$$
Donc l'EMM est d\'efini uniquement si la moyenne de l'\'echantillon $\bar{a}_n$ est comprise entre 0.5 et 1. 

Consistance. En appliquant le Lemme de l'application Continue (LAC).
En prenant $h(x) = 2 x - 1$, pour $1/2 < x <1$. La fonction est continue. On a \'egalement, $\bar{a}_n \xrightarrow[n \to +\infty]{P} E[X_1]$ selon la loi des grands nombres. Donc $\tilde{a}_n = h(\bar{a}_n) \xrightarrow[n \to +\infty]{P} h(E[X_1]) = a$. Donc consistance.

En appliquant le Th\'eo\`eme Central Limite (TCL) avec $\mu = a$ et $\sigma^2 = \frac{(1-a)^2}{12}$ on a  
$$
\frac{\sqrt{n}(\bar{a}_n - a)}{\sqrt{\sigma^2}} \xrightarrow[n \to +\infty]{P} Z \sim \mathcal{N}(0,1)
$$

\subsection*{Question 3}
On calcule
$$
\mathcal{L}_a(x_i,\ldots,x_n) = \prod_{1}^{n}{\frac{1}{1-a}}1_{x_i \in [a,1]}(x_i) =
\frac{1}{(1-a)^n}\prod_{1}^{n}{1_{x_i \in [a,1]}(x_i)} = \frac{1}{(1-a)^n}{1_{min(x_i) \leq x_i \leq 1}(x_i)}
$$

Ce qui donne la fonction suivante:
$$
\mathcal{L}_a(x_i,\ldots,x_n) =     
\left\{
    \begin{array}{cc}
        1 &  a = 0 \\
        \frac{1}{(1-a)^n} & 0 < a \leq min(x_i)\\
        0 & min(x_i) < a < 1 \\
    \end{array}
\right.
$$ 

$\mathcal{L}_a(x_i,\ldots,x_n)$ est croissante sur $0 \leq a \leq min(x_i)$ et nulle quand $min(x_i) < a$ donc EMV est maximale lorsque $a=min(x_i)$ donc $\hat{a}=min(x_i)$.  


$$
P(Z_{n} \geq s) = P\left(\frac{n(\hat{a}-a)}{1-a} \geq s\right) = P\left(\hat{a} \geq \frac{s(1-a)+na}{n}\right) 
$$.


$$
P(\hat{a} \geq t) = P(\forall i \in [1,n] X_i \geq t) = P(X_1 \geq t)^n = \left(\int_{t}^{1}{\frac{1}{1-a}dy}\right)^n = \left(\frac{1-t}{1-a}\right)^n
$$
Donc
$$
P(Z_{n} \geq s) = \left(\frac{1-\frac{s(1-a)+na}{n}}{1-a}\right)^n = \left(\frac{\frac{n-s(1-a)-na}{n}}{1-a}\right)^n = \left(\frac{(n-s)(1-a)}{n(1-a)}\right)^n = \left(\frac{n-s}{n}\right)^n
$$

Convergence en loi: $\lim_{n \to \infty} F_{X_n}(x) \to F_X(x)$.
$$
\lim_{n \to \infty} F_{Z_n}(x) = \lim_{n \to \infty} P({Z_n} \leq x) = \lim_{n \to \infty} 1- P({Z_n} \geq x) = 1 - \lim_{n \to \infty} P({Z_n} \geq x) = 1 - \lim_{n \to \infty} \left(\frac{n-x}{n}\right)^n
$$

On a $n-s <n$, donc $\lim_{n \to \infty} \left(\frac{n-x}{n}\right)^n \to 0$. Donc $\lim_{n \to \infty} F_{X_n}(x) \to 1$. 
Ce n'est pas une convergence en loi mais une convergence en probabilit\'e. ???? 


\subsection*{Question 4}
Vitesse de convergence de $\tilde{a}_n$.
Il faut trouver le plus grand $d$ qui verifie:
$$
n^d(\tilde{a}_n-a) \xrightarrow[n \to +\infty]{P} 0
$$
$$
\forall \epsilon, \lim_{n \to \infty}{\mathbb{P}(n^d(\tilde{a}_n-a) \geq \epsilon)} = 0
$$
$$
\forall \epsilon, \lim_{n \to \infty}{\mathbb{P}((2\bar{a}_n) \geq \frac{\epsilon}{n^d} +a+1)} = 0
$$

Je ne comprends rien.

\subsection*{Exercice 6}
\subsection*{Question 1}
\subsection*{Question 1-a}

en prenant $k=1$ on a 
$$\mathbb{E}(U^{2k}) = \frac{(2k)!}{2^kk!} = \frac{2!}{2.1!} = \frac{1}{2}$$

en prenant $k=2$ on a 
$$\mathbb{E}(U^{2k}) = \frac{(2k)!}{2^kk!} = \frac{4!}{4.2!} = \frac{3}{2}$$

\subsection*{Question 1-b}
Soit une variable al\'eatoire $Y = X_i^2$, l'\'ev\'enement $A = (Y \leq y)$ est \'equivalent \`a l'\'ev\'enement $B = (-\sqrt{y} \leq X \leq \sqrt{y})$. lorsque $y < 0$, on a $P(Y \leq y)=0$.
Lorsque $y \geq 0$,
$$
F_{Y}(y) = P( (Y \leq y)) = P (-\sqrt{y} \leq X \leq \sqrt{y}) = F_{x}(\sqrt{y}) - F_{x}(-\sqrt{y})
$$
et
$$
f_{y}(y) = \frac{\partial}{\partial y} F_{Y}(y) = \frac{\partial}{\partial y} F_{X}(\sqrt{y}) - \frac{\partial}{\partial y} F_{X}(-\sqrt{y}) = \frac{1}{2\sqrt{y}}[f_{x}(\sqrt{y}) + f_{x}(-\sqrt{y})]
$$

Dans notre cas, $f_{X}(x)$ est d\'efini pour $x \geq 0$, donc $f_{Y}(y)=\frac{1}{2\sqrt{y}}f_{x}(\sqrt{y})$. Ce qui fait~:

$$
f_{Y}(y) = \frac{1}{2\sqrt{y}} \left[ \frac{\sqrt{y}}{\theta}exp \left( -\frac{\sqrt{y}^2}{2\theta} \right) \right] = \frac{1}{2\theta} exp \left( -\frac{y}{2\theta} \right)
$$

qui est une loi exponentielle de param\`etre $\frac{1}{2\theta}$.

\subsection*{Question 1-c}
Calcul de $E(X_1)$
$$
E(X_1) = \int_{0}^{+\infty} x f_{x}(x) dx = \int_{-\infty}^{+\infty} x f_{X}(x) dx
$$
car $f_{X}(x) = 0$ lorsque $x < 0$.

$$
E(X_1) = \int_{-\infty}^{+\infty} \frac{x^2}{\theta} exp \left( -\frac{x^2}{2\theta}\right) dx
$$
Par substitution prenant $u = \frac{x}{\sqrt{\theta}}$, $\frac{\partial u}{\partial x} = \frac{1}{\sqrt{\theta}}$ et ${\partial x} = \sqrt{\theta}{\partial u}$, on obtient~:
$$
E(X_1) = \int_{-\infty}^{\infty} u^2 exp \left( -\frac{u^2}{2}\right) \sqrt{\theta} du = \sqrt{\theta} \frac{\sqrt{2\pi}}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u^2 exp \left( -\frac{u^2}{2}\right)  du 
$$
D'apr\`es la question 1-a, pour $k=1$, on a~:
$$
E(X_1) = \sqrt{2\pi\theta}\frac{(2k)!}{2^k k!} = \frac{\sqrt{2\pi\theta}}{2} = \sqrt{\frac{\pi\theta}{2}}
$$

Calcul de $E(X_1^2)$
$$
E(X_1^2) = \frac{1}{\frac{1}{2\theta}} = {2\theta}
$$
car $E(X_1^2)$ est une loi exponentielle de param\`etre $\frac{1}{2\theta}$.


Calcul de $E(X_1^3)$
$$
E(X_1^3) = \int_{0}^{+\infty} x^3 f_{x}(x) dx = \int_{-\infty}^{+\infty} x^3 f_{X}(x) dx
$$
car $f_{X}(x) = 0$ lorsque $x < 0$.

$$
E(X_1^3) = \int_{-\infty}^{+\infty} \frac{x^4}{\theta} exp \left( -\frac{x^2}{2\theta}\right) dx
$$
Par substitution prenant $u = \frac{x}{\sqrt{\theta}}$, $\frac{\partial u}{\partial x} = \frac{1}{\sqrt{\theta}}$ et ${\partial x} = \sqrt{\theta}{\partial u}$, on obtient~:
$$
E(X_1^3) = \int_{-\infty}^{\infty} \theta^2 u^4 exp \left( -\frac{u^2}{2}\right) \sqrt{\theta} du = \sqrt{\theta}\theta^2 \frac{\sqrt{2\pi}}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u^4 exp \left( -\frac{u^2}{2}\right)  du 
$$
D'apr\`es la question 1-a, pour $k=2$, on a~:
$$
E(X_1^3) = \sqrt{2\pi\theta}\theta^2\frac{(2k)!}{2^k k!} = \frac{3\sqrt{2\pi\theta}\theta^2}{2} 
$$

Calcul de $E(X_1^4)$.
$$
E(X_1^4) = \int_{0}^{+\infty} x^4 f_{x}(x) dx = \int_{0}^{+\infty} \frac{x^5}{\theta} exp \left( -\frac{x^2}{2\theta}\right) dx = 8\theta^2 
$$
Par utilisation d'un solveur internet, mais faisable \`a la main.

\subsection*{Question 2}
\subsection*{Question 2-a}
On calcule
$$
\mathcal{L}_{\theta}(x_i,\ldots,x_n) = 
\prod_{1}^{n}\frac{x^2}{\theta} exp \left( -\frac{x^2}{2\theta}\right) 1_{x_i \in [0,+\infty]}(x_i) =
\frac{x_1x_2 \ldots x_n}{\theta^n} exp \left( -\frac{x_1^2+x_2^2+ \ldots +x_n^2 }{2\theta} \right) \prod_{1}^{n}{1_{x_i \in [0,+\infty]}(x_i)}
$$

On calcule $\frac{\partial}{\partial \theta} \mathcal{L}_{\theta}(x_i,\ldots,x_n)$ et on cherche  les points o\'u la d\'eriv\'ee s'annule. 

En utilisant un solveur internet on a 
$$
\frac{\partial}{\partial \theta} \mathcal{L}_{\theta}(x_i,\ldots,x_n) = \frac{x_1 x_2 \ldots x_n \theta^{-n-2}.(2n\theta - (x_1+x_2+\ldots + x_n)) e^{-\frac{x_1^2+x_2^2+ \ldots + x_n^2}{2\theta}}} {2}
$$

Et
$$
\frac{\partial}{\partial \theta} \mathcal{L}_{\theta}(x_i,\ldots,x_n) = 0
$$
quand $\hat{\theta_n} = \frac{(x_1^2+x_2^2+\ldots + x_n^2)}{2n}$.

Ou plus simple (en passant par le log car $\mathcal{L}_{\theta}(x_i,\ldots,x_n) \neq 0$)
$$
\frac{\partial}{\partial \theta} \ln(\mathcal{L}_{\theta}(x_i,\ldots,x_n)) = \frac{\partial}{\partial \theta} \left( ln(x_1x_2 \ldots x_n) - ln(\theta^n)  -  \frac{x_1^2+x_2^2+ \ldots + x_n^2}{2\theta} \right) = -\frac{n}{\theta} + \frac{x_1^2+x_2^2+ \ldots + x_n^2}{2\theta^2}
$$

Et
$$
-\frac{n}{\theta}+\frac{x_1^2+x_2^2+ \ldots + x_n^2}{2\theta^2} = \frac{-2n\theta + x_1^2+x_2^2+ \ldots + x_n^2}{2\theta^2} = 0
$$
quand $\hat{\theta_n} = \frac{x_1^2+x_2^2+\ldots + x_n^2}{2n}$.

\subsection*{Question 2-b}
Le biais de $\hat{\theta}$ est~:
$$
b(\hat{\theta}) = E(\hat{\theta}) - \theta = E\left(\frac{x_1^2+x_2^2+\ldots + x_n^2}{2n}\right) - \theta = \frac{1}{2n}(E(x_1^2) + E(x_2^2) + \ldots + E(x_n^2)) - \theta
$$
M\^eme loi, donc tous les $E(x_i^2)$ sont identiques. 
$$
=\frac{1}{2n}(2\theta + 2\theta + \ldots + 2\theta) - \theta = \frac{2n\theta}{2n} - \theta = 0
$$

Le risque quadratique de $\hat{\theta}$ est~:
$$
r(\hat{\theta}) = E((\hat{\theta} - \theta)^2) = b(\hat{\theta}) + V(\hat{\theta}) = 0 + V(\hat{\theta}) = E(\hat{\theta}^2) - (E(\hat{\theta}))^2
$$

\subsection*{Question 2-c}

\subsection*{Question 2-d}

\subsection*{Question 3}
\subsection*{Question 3-a}
\subsection*{Question 3-b}


\subsection*{Question 4}
\subsection*{Question 4-a}

$$
E(\bar{U}^2) = E\left(\left(\frac{1}{n} \sum_{i=1}^{n}U_i \right)^2\right) = \frac{1}{n^2} E\left(\left(\sum_{i=1}^{n}U_i \right)^2\right) 
$$
$$
= \frac{1}{n^2} E\left(\sum_{i=1}^{n}U_i^2 + 2\sum_{1 \leq i < j \leq n} U_i U_j\right) = \frac{1}{n^2} \left(\sum_{i=1}^{n}E(U_i^2) + 2 \sum_{1 \leq i < j \leq n} E(U_i U_j) \right)
$$
Comme les $U_i$ suivent la m\^eme loi, elles ont la m\^eme esperance, de plus elles sont independantes donc $E(U_iU_j) =E(U_i)E(U_j)$. Donc

$$
E(\bar{U}^2) = \frac{1}{n^2} \left(nE(U_1^2) + 2 \sum_{1 \leq i < j \leq n} (E(U_1))^2 \right)
$$
On a aussi $E(X^2) = V(X) + (E(X))^2$. Donc
$$
E(\bar{U}^2) = \frac{1}{n^2} \left(n(V(U_1) + (E(U_1))^2) + 2 \sum_{1 \leq i < j \leq n} (E(U_1))^2 \right) = \frac{1}{n^2} \left(n \left(V(U_1) + (E(U_1))^2 \right) + 2 \frac{n(n-1)}{2} (E(U_1))^2 \right)
$$
$$
= \frac{1}{n^2} \left(nV(U_1) + n(E(U_1))^2 + n(n-1) (E(U_1))^2 \right) = \frac{1}{n^2} \left(nV(U_1) + n^2 (E(U_1))^2 \right) = (E(U_1))^2 +\frac{V(U_1)}{n} 
$$

La seconde partie, on admet.

\subsection*{Question 4-b}




\end{document}

